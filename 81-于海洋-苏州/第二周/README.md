# 机器学习的数学基础

## 矩阵乘法
A矩阵xB矩阵 = A的列数x B的行数
## 单位矩阵
单位矩阵是一个n×n矩阵，从左到右的对角线上的元素是1，其余元素都为0。
```
1 0       1 0 0
0 1       0 1 0
          0 0 1
```

如果A是n×n矩阵，I是单位矩阵，则A * I= A, I * A = A
相当于 普通数学中的1

## 逆矩阵
矩阵A的逆矩阵记作A-1， A * A-1 = A-1 * A = I，I是单位矩阵


## 奇异矩阵
当一个矩阵没有逆矩阵的时候，称该矩阵为奇异矩阵。 ->当且仅当一个矩阵的行列式为零时，该矩阵是奇异矩阵。
当ad-bc=0时，|A|没有定义，A-1不存在，A是奇异矩阵。
```
1  2
1  2
```
## 转置矩阵
简单地说，矩阵的转置就是行列互换，用AT表示A的转置矩阵。
## 对称矩阵

如果一个矩阵转置后等于原矩阵，那么这个矩阵称为对称矩阵。
一个**矩阵转置**和这个**矩阵**的乘积就是一个**对称矩阵**。

## 欧式变换
欧氏变换由两部分组成: 
- 旋转
- 平移

## 齐次坐标
简而言之，齐次坐标就是用N+1维来代表N维坐标
我们可以在一个2D坐标末尾加上一个额外的变量w来形成2D齐次坐标 因此，一个点(X,Y)在齐次坐标里面变成了(x,y,w)，并且有
X = x/w   Y = y/w

例如:
(1，2)的齐次坐标可以表示为(1，2，1)

如果点(1，2)移动到无限远处，在笛卡尔坐标下它变为(∞,∞)，然后它的齐次坐标表示 为(1，2，0)，因为(1/0, 2/0) = (∞,∞)，我们可以不用”∞"来表示一个无穷远处的点了


## 导数&偏导数 
- 导数(微分、斜率)
是代表函数(曲线)的斜率，是描述函数(曲线)**变化快慢的量**，同时曲
线的极大值点也可以使用导数来判断，即极大值点的导数为0，此时斜率为零。
- 偏导数
是指在多元函数的情况下，对其++每个变量进行求导++，求导时，把其他变量看做常 量进行处理，物理意义就是查看这一个变量在其他情况不变的情况下对函数的影响程度。

## 梯度
梯度: 梯度的本意是一个**向量(矢量)**，表示某一函数在该点处的方向导数沿着该方向取得最大 值，即**函数在该点处沿着该方向(此梯度的方向)变化最快，变化率最大(为该梯度的模)**
简而言之，对多元函数的各个自变量求偏导数，并把求得的这些偏导数写成向量形式，就是梯
度。


- 梯度下降法
是一种寻找函数极小值的方法。 该方法最普通的做法是:在已知参数当前值的情况下，按当前点对应的**梯度向量的反方
向**，并按事先给定好的步长大小，对参数进行调整。 按如上方法对参数做出多次调整之后，函数就会逼近一个极小值。 


## 数学期望、方差、标准差
- 数学期望(均值)
表示一件事平均发生的概率，记为E(x), E(x) = x1p1+x2p2+...+ xnpn
- 方差
用来刻画随机变量x和数学期望E(x)之间的偏离程度，记做D(x)

- 标准方差
标准方差是方差的算术平方根。标准差能反映一个数据集的离散程度

## 正态分布
若随机变量X服从一个数学期望为μ、方差为σ^2的正态分布，记为N(μ，σ^2)。 **μ决定了其位置(中心线)**，其**标准差σ决定了分布的幅度(胖瘦)。**
标准正态分布:当μ = 0，σ = 1时的正态分布是标准正态分布

## 熵
信息量:信息量是指信息多少的度量。


# 数字图像
## GRB转Gray

| 方式   | 算法                               |
|------|----------------------------------|
| 浮点算法 | Gray =  R*0.3 + G*0.59 + B*0.11  |
| 整数算法 | Gray = （R*30 + G*59 + B*11）/ 100 |
| 位移方法 | Gray = （R*76 + G*151 + B*28）>>8  |
| 平均值法 | Gray = （R + G +B  / 3            |
| 仅取绿色 | Gray =  G                        |


- 转浮点数的原因
浮点数运算更精确，整数运算中会因丢失小数部分导致颜色失真。 **计算次数越多越失真**

## 图像取样
略