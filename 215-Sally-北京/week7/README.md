### 深度学习与神经网络
1. 深度学习
    1. 概论
        1. 机器学习是人工智能的核心，研究如何使用机器来模拟人类学习活动的一门学科。
        2. 深度学习（多层人工神经网络）是机器学习的分支，对数据进行表征学习的方法。
        3. 人工智能关系圈：以下3个，前面的包含后面的
            1. 机器学习
                1. 种实现人工智能的方法
                2. 使用归纳、综合，而不是演绎
                3. 分类
                    1. 传统机器学习算法：分类、聚类等等很多
                    2. 深度学习算法
            2. 深度学习：一种实现机器学习的技术
            3. 人工神经网络：：一种机器学习的算法。
    2. 神经网络
        1. 人是怎么思考的？--生物神经网络
        2. 人工神经网络分为两个阶段：
            1. 预激活阶段：接受来自其他n个神经元传递过来的信号，这些输入信号通过与相应的权重进行加权求和传递给下个阶段
            2. 把预激活的加权结果传递给激活函数
        3. 与生物神经网络的类比：
            1. 细胞核 - 神经元
            2. 树突 - 输入
            3. 轴突 - 输出
            4. 突触 - 权重
        4. 机器是怎么思考的？--人工智能网络
        5. 神经网络组成
            1. 神经元
                1. 模仿人体的神经元
                2. 公式：sum = WX + b
                    1. W、X都是矩阵
                    2. W代表权重，X是输入，WX代表权重累加过程
                    3. b是偏移量矩阵
                3. 重要的2个部分：
                    1. 输入：是特征向量。特征向量代表的是变化的方向。或者说，是最能代表这个事物的特征的方向。
                        1. 卷积提取的特征，就可以作为神经元的输入，否则全部像素点作为输入计算量太多了
                    2. 权重（权值）：就是特征值。有正有负，加强或抑制，同特征值一样。权重的绝对值大小，代表了输入信号对神经元的影响的大小。
                4. 神经元本质：一刀切，非黑即白
                    1. 神经元就是当h大于0时输出1，h小于0时输出0这么一个模型，它的实质就是把特征空间一切两半，认为两半分别属于两个类。
                5. 单个神经元的缺点：只能一刀切(怎么切取决于w和b)
                6. 解决方法：多层神经网络
            2. 神经网络
                1. 简介：
                    1. 神经网络是一种运算模型，由大量的节点（神经元）和之间相互的联接构成
                    2. 每两个节点间的联接都代表一个对于通过该连接信号的加权值，称之为权重，这相当于人工神经网络的记忆。
                    3. 网络的输出则依网络的连接方式、权重值和激励函数的不同而不同。而网络自身通常都是对自然界某种算法或者函数的逼近，也可能是对一种逻辑策略的表达。
                    4. 单层神经网络（感知器）
                2. 多层神经网络
                    1. 神经网络是由多个神经元组合而成，前一个神经元的结果作为后一个神经元的输入，神经网络一般分为三层，第一层作为输入层，最后一层作为输出层，中间的全部是隐含层。依次组合而成。
                        1. 同层节点没有连接
                        2. 输入层可以接收图片的像素点
                        3. 输出层的节点个数和需求相符，如使用one hot编码方式
                    2. 理论证明，任何多层网络可以用三层网络近似地表示。
                    3. 一般凭经验来确定隐藏层到底应该有多少个节点，在测试的过程中也可以不断调整节点数以取得最佳效果。
                        1. 隐藏层的个数、层数都与输入和输出无关
                        2. 隐藏层的节点个数和隐藏层的层数的调整，要考虑算力
                3. 前馈神经网络
                    1. 人工神经网络模型主要考虑网络链接的拓扑结构、神经元特征、学习规则等。
                    2. 其中，前馈神经网络也称为多层感知机
                    3. 从输入到输出的一个正向、单向的过程
                    4. 结构
                        1. 输入层--固定的一层
                        2. 隐藏层--不固定的N层，参数调优调的就是隐藏层
                        3. 输出层--固定的一层
                4. 激活函数
                    1. 地位：激活函数是神经网络设计的一个核心单元。
                    2. 状态：
                        1. 激活
                        2. 抑制
                    3. 作用：为了在神经网络中引入非线性的学习和处理能力。
                    4. 激活函数需满足的条件：
                        1. 非线性
                            1. 为什么要把直线变成曲线？--数据分布没有那么的规则，用一条直线分不开，会漏掉几个点，用一条曲线就可以分开
                            2. 如果wx+b是一把直的刀，那么激活函数是一把曲线的刀
                        2. 可微性：可导可微
                        3. 单调性
                    5. 常用激活函数：（函数图像类似矩形脉冲）
                        1. sigmoid函数，过点(0, 0.5)，在0 到 1之间
                            1. 缺点：
                                1. 梯度饱和，看图可知，两边数值的梯度都为0；
                                2. 结果的平均值不为0，这是我们不希望的，因为这会导致后层的神经元的输入是非0均值的信号，这会对梯度产生影响。
                                3. 梯度弥散
                            2. 优点
                                1. 计算复杂度低，推理时间短
                        2. tanh函数，过点(0, 0.)，在-1 到 1之间
                            1. tanh(x) = 2σ(2x) - 1
                            2. 与σ对比：我们更追求推理时间更短
                                1. 训练时间：sigmoid > tanh
                                2. 推理时间：tanh > sigmoid
                            3. 缺点：
                                1. 梯度弥散没解决
                            4. 优点：
                                1. 解决了原点对称问题
                                2. 训练阶段比sigmoid更快
                        3. ReLU函数 f(x) = max (0, x)
                            1. 每一段是线性，但整体横折横折就是非线性的了
                            2. 最受欢迎的激活函数
                                1. 模拟人脑神经元的激活模型
                                2. 简单粗暴
                            3. 缺点
                                1. 梯度弥散没完全解决，在(-)部分相当于神经元死亡且不会复活
                            2. 优点：
                                1. 解决了部分梯度弥散问题
                                2. 收敛速度更快
                5. 神经元稀疏
                    1. 稀疏的原因：
                        1. ReLU 把所有的负值都变为0，而正值不变，这种操作被称为单侧抑制。正因为有了这单侧抑制，才使得神经网络中的神经元也具有了稀疏激活性。
                        2. 当模型增加N层之后，理论上ReLU神经元的激活率将降低2的N次方倍。
                        3. 为什么要做这种稀疏？
                            1. 模拟人脑神经元工作方式，人类大脑神经元也是有稀疏方式的
                            2. 大脑在思考时，同时被激活的神经元占总数的 1% - 4%，其他神经元处于被抑制状态
                            3. 减少计算量
                6. 张量（tensor）
                    1. tensor是一种数据结构
                    2. 张量一大特征是维度，在Python中，一个张量的维度可以通过读取它的.ndim属性来获取，常见的模型张量：(n, h, w, c)
                        1. 一个0维张量就是一个常量
                        2. 数组就等价与一维张量
                        3. 一个二维数组就是一个二维张量
                        4. 所谓n维张量，其实就是一维数组，数组中的每个元素都是n-1维张量（即多维数组的嵌套）
                            1. 由此可见，3维张量其实就是一个一维数组，数组中的每个元素就是2维数组。
                    3. 一个n维张量经常用一组数据来表示，它可以用(3,2,2)这组数据结合来表示，一个张量是几维度，那么括号里面就有几个数字。
                        1. (3,2,2)的意义：
                            1. 最外层是3个
                            2. 2：再里面一层有两个二位张量
                            3. 2：二维张量中含有两个常量(1维张量)
            3. 设计神经网络
                1. 概论：
                    1. 使用神经网络训练数据之前，必须先确定
                        1. 神经网络的层数
                            1. 输入、输出层都是固定的1层，单元个数也相对好确定，即特征数、像素点数等
                            2. 隐藏层相对不好确定，没有公式，可以根据实验测试和误差以及精确准确度来实验并改进
                        2. 每层的单元个数
                    2. 特征向量在被传入输入层时通常要先标准化到0-1之间，为了加速学习过程
                    3. 离散型变量可以被编码成每一个输入单元对应一个特征值可能赋的值，如one hot编码
                    4. 神经网络可以用来做分类问题，也可以解决回归问题
                2. 对隐藏层的感性认识
                    1. 拆解成一个个针对特征值的小问题
                    2. 每一个小问题还可以继续拆成更小的问题
                    3. 直到每一个问题可以用一个单独的神经元被回答
                    4. 隐藏层数和节点数越多判断的准确性，但也要考虑算力和存储，trade off
                    5. 先用卷积提取特征后才给隐藏层
                    6. 两个关键点：
                        1. 不能一味的增加隐藏层和节点个数
                        2. 隐藏层判断用的 w 是计算机自己算出来的，不能人为指定，人类只能指定最后的那个大问题
                    7. 类似于解答数学大题，可以有多种方法求解，但是只看最后答案是对的就行
            4. 深度学习 - 什么是深度学习？
                1. 深度学习，就是多层人工神经网络
                2. 深度学习最重要的作用是表征学习，学习曾计划的特征，“深度”指的就是多层
                3. 例如：
                    1. 图像识别：像素->边缘->纹理->图形->局部->物体
                    2. 文字识别：字符->词->词组->子句->句子->故事
                4. 关系：人工智能(AI) > 机器学习(ML) > 深度学习(DL)
                5. 数据规模推动深度学习进图
    3. 推理和训练 -- 通俗的说就是找w
        1. 监督学习与非监督学习 -- 区别是有没有标签
            1. Supervised Learning 有监督式学习
                1. 数据有标签
                2. 如果错误了再进行修正
                3. 训练过程一直持续到基于训练数据达到预期的精确性
                4. 关键方法：
                    1. 分类
                    2. 回归
                5. 例如：
                    1. 逻辑回归(Logistic Regression)
                    2. BP神经网络(Back Propagation Neural Network)
                    3. 深度学习
            2. Unsupervised Learning 无监督学习:
                1. 无标签
                2. 基于没有标记的输入数据采取推导结构的模型
                3. 关键方式：
                    1. 关联规则学习和
                    2. 聚合
                4. 例如：k-means
        2. 深度学习的推理和训练
            1. 训练（Training）
                1. 定义：一个初始神经网络通过不断的优化自身参数（即确定w），来让自己变得准确。这整个过程就称之为训练（Traning）
                2. 类比刷高考题
            2. 推理（Inference）
                1. 定义：你训练好了一个模型，在训练数据集中表现良好，但是我们的期望是它可以对以前没看过的图片进行识别。你重新拍一张图片扔进网络让网络做判断，这种图片就叫做现场数据（livedata），如果现场数据的区分准确率非常高，那么证明你的网络训练的是非常好的。这个过程，称为推理（Inference）。
                2. 类比模拟考试和真正的高考
                3. 过程和训练的正向传播过程一模一样
        3. 优化和泛化 -- 深度学习的根本问题是优化和泛化之间的对立
            1. 优化（optimization）是指调节模型以在训练数据上得到最佳性能（即机器学习中的学习）
                1. 即训练过程
            2. 泛化（generalization）是指训练好的模型在前所未见的数据上的性能好坏
                1. 即推理过程
        4. 数据集
            1. 分类：
                1. 训练集：实际训练算法的数据集；用来计算梯度，并确定每次迭代中网络权值的更新
                2. 验证集：用于跟踪其学习效果的数据集；是一个指示器，用来表明训练数据点之间所形成的网络函数发生了什么，并且验证集上的误差值在整个训练过程中都将被监测
                    1. 类似于模拟考试
                    2. 此时训练还没有结束
                3. 测试集：用于产生最终结果的数据集
                    1. 类似于高考
                    2. 此时训练已结束
            2. 为了让测试集能有效反映网络的泛化能力：
                1. 测试集绝不能以任何形式用于训练网络，即使是用于同一组备选网络中挑选网络。测试集只能在所有的训练和模型选择完成后使用；
                    1. 即训练时不能污染测试集
                2. 测试集必须代表网络使用中涉及的所有情形。
                    1. 即考试不能超纲
            3. 交叉验证
                1. 这里有一堆数据，我们把他切成3个部分（当然还可以分的更多）
                    1. 第一部分做测试集，二三部分做训练集，算出准确度；
                    2. 第二部分做测试集，一三部分做训练集，算出准确度；
                    3. 第三部分做测试集，一二部分做训练集，算出准确度；
                2. 之后算出三个准确度的平局值，作为最后的准确度。
        5. bp神经网络
            1. 简介
                1. BP网络（Back-Propagation Network）是1986年被提出的，是一种按误差逆向传播算法训练的多层前馈网络，是目前应用最广泛的神经网络模型之一，用于函数逼近、模型识别分类、数据压缩和时间序列预测等。
                2. BP网络又称为反向传播神经网络，它是一种有监督的学习算法，具有很强的自适应、自学习、非线性映射能力，能较好地解决数据少、信息贫、不确定性问题，且不受非线性模型的限制。
                3. 一个典型的BP网络应该包括三层:输入层、隐藏层和输出层。各层之间全连接，同层之间无连接。隐藏层可以有很多层。
            2. 神经网络的训练
                1. 我们利用神经网络去解决图像分割，边界探测等问题时候，我们的输入（假设为x），与期望的输出（假设为y）之间的关系究竟是什么？也就是y=f(x)中，f是什么，我们也不清楚，但是我们对一点很确信，那就是f不是一个简单的线性函数，应该是一个抽象的复杂的关系，那么利用神经网络就是去学习这个关系，存放在model中，利用得到的model去推测训练集之外的数据，得到期望的结果。
            3. 训练（学习）过程：
                1. 正向传播：输入信号从输入层经过各个隐藏层向输出层传播。在输出层得到实际的响应值，若实际值与期望值误差较大，就会转入误差反向传播阶段。
                2. 反向传播：按照```梯度下降法```从输出层经过各个隐含层并逐层不断地调整各神经元的连接权值和阈值，反复迭代，直到网络输出的误差减少到可以接受的程度，或者进行到预先设定的学习次数。
        6. 神经网络的训练
            1. 概念
                1. 代(Epoch)：使用训练集的全部数据对模型进行一次完整训练，被称为“一代训练”。
                2. 批大小(Batch size)：使用训练集的一小部分样本对模型权重进行一次反向传播的参数更新，这一小部分样本被称为“一批数据”
                3. 迭代(Iteration)
                    1. 使用一个Batch数据对模型进行一次参数更新的过程，被称为“一次训练”（一次迭代）。每一次迭代得到的结果都会被作为下一次迭代的初始值。
                    2. 一个迭代=一个正向通过+一个反向通过。
                4. 比如训练集有500个样本，batchsize = 10 ，那么训练完整个样本集：iteration=50，epoch=1.
            2. 训练过程：1-2步是设计，后面才开始训练
                1. 提取特征向量作为输入。
                2. 定义神经网络结构。包括隐藏层数，激活函数等等。
                3. 通过训练利用反向传播算法不断优化权重的值(w)，使之达到最合理水平。
                4. 使用训练好的神经网络来预测未知数据(推理)，这里训练好的网络就是指权重达到最优的情况
            3. 详细训练过程
                1. 选择样本集合的一个样本（Ai，Bi），Ai为数据、Bi为标签（所属类别）
                2. 送入网络，计算网络的实际输出Y，（此时网络中的权重应该都是随机量）
                3. 计算D=Bi−Y（即预测值与实际值相差多少）
                    1. 这里的减号并不是真的数学上的减法，而是求误差
                4. 根据误差D调整权重矩阵W
                5. 对每个样本重复上述过程，直到对整个样本集来说，误差不超过规定范围（或迭代次数）
            4. 更具体的训练过程：--整个过程没有人为设定，都是计算机自己进行的
                1. 参数的随机初始化，即权重的随机初始化
                2. 前向传播计算每个样本对应的输出节点激活函数值
                3. 计算损失函数
                4. 反向传播计算偏导数
                5. 使用```梯度下降```或者先进的优化方法更新权值
            5. 更具体的过程具体：
                1. 参数的随机初始化
                    1. 对于所有的参数我们必须初始化它们的值，而且它们的初始值不能设置成一样，比如都设置成0或1。如果设置成一样那么更新后所有参数都会相等。即所有神经元的功能都相等，造成了高度冗余。所以我们必须随机化初始参数。
                    2. 特别的，如果神经网络没有隐藏层，则可以把所有参数初始化为0。（但这也不叫深度神经网络了）
                2. 标准化（也叫归一化）-- 非必须步骤
                    1. 原因：去单位，变成无量纲的纯数值，便于不同单位或量级的指标能够进行比较和加权
                    2. 归一化处理方法：
                        1. 归一化：
                            1. 减去最小值值求平均法：y=(x - min) / (max - min)，这样得到的y在[0,1]区间上?
                            2. 减去平均值求平均法：y=(x - mean) / (max - min)，这样得到的y在[-1,1]区间上?
                        2. z-score标准化（零均值归一化zero-mean normalization）：y = (x - μ) / σ
                            1. 经过处理后的数据均值为0，标准差为1（正态分布）
                            2. 其中μ是样本的均值， σ是样本的标准差
                3. 损失函数 -- 必须步骤
                    1. 概念：损失函数用于描述模型预测值与真实值的差距大小
                    2. 两种常见的方法：
                        1. 均值平方差（Mean Squared ErroMSE），也称均方误差
                        2. 交叉熵（cross entropy）
                            1. 一般用在分类问题上
                            2. 表达意思为预测输入样本属于哪一类的概率。值越小，代表预测结果越准。公式见ppt
                        3. 损失函数的选取取决于输入标签数据的类型：
                            1. 如果输入的实数、无界的值，损失函数使用MSE。
                            2. 如果输入标签是位矢量（分类标志）（虚数），使用交叉熵会更适合。
                4. 梯度下降法
                    1. 应用：利用反向传播和梯度下降法计算新的 w，希望损失函数r越小越好
                    2. 定义：梯度∇f=(∂x1 ∂f ;∂x2 ∂f ;…;∂xn ∂f )指函数关于变量x的导数
                        1. 梯度的方向表示函数值增大的方向
                            1. 因为希望损失函数越来越小
                            2. 所以需要将参数值向着梯度的反方向调整
                        2. 梯度的模表示函数值增大的速率
                    3. 那么只要不断将参数的值向着梯度的反方向更新一定大小，就能得到函数的最小值（全局最小值或者局部最小值）
                    4. 一般利用梯度更新参数时会将梯度乘以一个小于1的学习速率（learning rate），这是因为往往梯度的模还是比较大的，直接用其更新参数会使得函数值不断波动，很难收敛到一个平衡点（这也是学习率不宜过大的原因）。
                5. 学习率（learning rate）也叫步长
                    1. 学习率是一个重要的超参数，它控制着我们基于损失梯度调整神经网络权值的速度。
                    2. 学习率越小，我们沿着损失梯度下降的速度越慢。
                    3. 从长远来看，这种谨慎慢行的选择可能还不错，因为可以避免错过任何局部最优解，但它也意味着我们要花更多时间来收敛，尤其是如果我们处于曲线的至高点。
                    4. 调整学习率是算法工程师一个很重要的工作，一般控制在0.1 至 0，8之间
                    5. 新权值 = 当前权值 - 学习率 × 梯度 
                        1. 梯度是矢量，这里负号的意思是取梯度的反方向
                        2. 梯度哪来的？
                            1. 损失函数的梯度
                            2. 损失函数由 交叉熵 或 MSE 得来
