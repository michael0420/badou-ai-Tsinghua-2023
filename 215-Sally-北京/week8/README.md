### 从零开始训练网络
1. 泛化能力（即推理能力）的分类
    1. 欠拟合：模型没有能够很好的表现数据的结构，而出现的拟合度不高的情况。模型不能在训练集上获得足够低的误差；
        1. 2. 训练不达标，需要改模型或换训练数据
    2. 拟合：测试误差与训练误差差距较小；
        1. 达标
    3. 过拟合：模型过分的拟合训练样本，但对测试样本预测准确率不高的情况，也就是说模型泛化能力很差。训练误差和测试误差之间的差距太大；
        1. 需要重点处理
        2. 缺点：
            1. 造成模型比较复杂
            2. 模型的泛化性能差，正确率低。
        3. 原因：
            1. 建模样本选取了错误的选样方法、样本标签等，或样本数量太少，所选取的样本数据不足以代表预定的分类规则
            2. 样本噪音干扰过大，使得机器将部分噪音认为是特征从而扰乱了预设的分类规则
            3. 假设的模型无法合理存在，或者说是无法达到假设成立的条件
            4. 参数太多导致模型复杂度过高
            5. 对于神经网络模型：
                1. 对样本数据可能存在分类决策面不唯一，随着学习的进行,，BP算法使权值可能收敛过于复杂的决策面；
                2. 权值学习迭代次数足够多，拟合了训练数据中的噪声和训练样例中没有代表性的特征。
        4. 解决办法：
            1. 减少特征：删除与目标不相关特征，如一些特征选择方法
                1. 即降维
            2. Early stopping
                1. 在每一个Epoch结束时，计算validation data的accuracy，当accuracy不再提高时，就停止训练。
                2. 那么该做法的一个重点便是怎样才认为validation accurary不再提高了呢？并不是说validation accuracy一降下来便认为不再提高了，因为可能经过这个Epoch后，accuracy降低了，但是随后的Epoch又让accuracy又上去了，所以不能根据一两次的连续降低就判断不再提高。
                3. 一般的做法是，在训练的过程中，记录到目前为止最好的validation accuracy，当连续10次Epoch（或者更多次）没达到最佳accuracy时，则可以认为accuracy不再提高了。此时便可以停止迭代了（Early Stopping）。
                4. 这种策略也称为“No-improvement-in-n”，n即Epoch的次数，可以根据实际情况取，如10、20、30
            3. 更多的训练样本。
            4. 重新清洗数据。
                1. 即降噪，可以用滤波的方法
            5. Dropout算法
                1. 在神经网络中，dropout方法是通过修改神经网络本身结构来实现的：
                    1. 在训练开始时，随机删除一些（可以设定为1/2，也可以为1/3，1/4等）隐藏层神经元，即认为这些神经元不存在（并不是真的删除，训练完会恢复），同时保持输入层与输出层神经元的个数不变。
                    2. 然后按照BP学习算法对ANN中的参数进行学习更新（虚线连接的单元不更新，因为认为这些神经元被临时删除了）。这样一次迭代更新便完成了。下一次迭代中，同样随机删除一些神经元，与上次不一样，做随机选择。这样一直进行，直至训练结束。Dropout方法是通过修改ANN中隐藏层的神经元个数来防止ANN的过拟合。
                2. 为什么Dropout能够减少过拟合：-- 解决参数过多的过拟合
                    1. Dropout是随机选择忽略隐层节点，在每个批次的训练过程，由于每次随机忽略的隐层节点都不同，这样就使每次训练的网络都是不一样的， 每次训练都可以当做一个“新”模型；
                    2. 隐含节点都是以一定概率随机出现，因此不能保证每2个隐含节点每次都同时出现。这样权值的更新不再依赖有固定关系隐含节点的共同作用，阻止了某些特征仅仅在其他特定特征下才有效果的情况。
                3. 总结
                    1. Dropout是一个非常有效的神经网络模型平均方法，通过训练大量的不同的网络，来平均预测概率。不同的模型在不同的训练集上训练（每个epoch的训练数据都是随机选择），最后在每个模型用相同的权重来“融合”。
                    2. 一个dropout只管一层，多个dropout，每层可以删除不同比例的神经元
                4. 扩展：
                    1. 经过交叉验证，隐藏节点dropout率等于0.5的时候效果最好。
                    2. dropout也可以被用作一种添加噪声的方法，直接对input进行操作。输入层设为更接近1的数。使得输入变化不会太大（0.8）
                        1. 比较少见  
        4. 不收敛：模型不是根据训练集训练得到的。
            1. 指的是误差不收敛
            2. 训练不达标，需要改模型或换训练数据
2. bp神经网络
    1. 训练（学习）过程：
        1. 正向传播
            1. 输入信号从输入层经过各个隐藏层向输出层传播。在输出层得到实际的响应值，若实际值与期望值误差较大，就会转入误差反向传播阶段。
        2. 反向传播
            1. 按照梯度下降的方法从输出层经过各个隐含层并逐层不断地调整各神经元的连接权值和阈值，反复迭代，直到网络输出的误差减少到可以接受的程度，或者进行到预先设定的学习次数。
    2. BP算法是一个迭代算法，它的基本思想如下：
        1. 将训练集数据输入到神经网络的输入层，经过隐藏层，最后达到输出层并输出结果，这就是前向传播过程。
        2. 由于神经网络的输出结果与实际结果有误差，则计算估计值与实际值之间的误差，并将该误差从输出层向隐藏层反向传播，直至传播到输入层；
        3. 在反向传播的过程中，根据误差调整各种参数的值（相连神经元的权重），使得总损失函数减小。
            1. 梯度下降法
            2. 求偏导
        4. 迭代上述三个步骤（即对数据进行反复训练），直到满足停止准则。
            1. early stopping
            2. 或 lose 小于阈值
    3. 手推训练过程
3. 用Keras实现一个简单神经网络（代码实现）
    1. 概念：
        1. Keras是由纯python编写的基于theano/tensorflow的深度学习框架。
        2. Keras是一个高层神经网络API，支持快速实验，能够把你的idea迅速转换为结果，如果有如下需求，可以优先选择Keras：
            1. 简易和快速的原型设计（keras具有高度模块化，极简，和可扩充特性）
            2. 支持CNN和RNN，或二者的结合
            3. 无缝CPU和GPU切换
    2. Softmax：
        1. softmax用于多分类过程中，它将多个神经元的输出，映射到[0, 1]区间内，可以看成概率来理解，从而用来进行多分类
            1. softmax 函数图像、单调性符合激活函数要求
            2. 在Keras中，softmax可以直接添加在输出层的activation
